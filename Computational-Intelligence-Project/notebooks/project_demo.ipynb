{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b075e00",
   "metadata": {},
   "source": [
    "# Computational Intelligence Project: Custom NN Library\n",
    "## Part 1 Submission: Core Library Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from lib.network import Sequential\n",
    "from lib.layers import Dense, BaseLayer\n",
    "from lib.activations import ReLU, Tanh, Sigmoid\n",
    "from lib.losses import MeanSquaredError\n",
    "from lib.optimizer import SGD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Gradient Check Function ---\n",
    "def check_gradients(model, X, Y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Compares analytical gradients (from backprop) with numerical gradients (finite difference).\n",
    "    The relative error should be below 1e-7 for correctness.\n",
    "    \"\"\"\n",
    "    # 1. Run forward and backward pass to calculate analytical gradients (dA, db)\n",
    "    Y_pred = model.forward(X)\n",
    "    model.backward(Y_pred, Y)\n",
    "\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        # Only check layers with parameters (Dense)\n",
    "        if not hasattr(layer, 'A'):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nChecking Layer {idx+1} ({layer.__class__.__name__})...\")\n",
    "\n",
    "        # --- Check Weights (A) ---\n",
    "        num_grad_A = np.zeros_like(layer.A)\n",
    "        it = np.nditer(layer.A, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            i = it.multi_index\n",
    "            orig = layer.A[i]\n",
    "\n",
    "            # L(W + epsilon)\n",
    "            layer.A[i] = orig + epsilon\n",
    "            L_plus = model.loss_function.loss(model.forward(X), Y)\n",
    "\n",
    "            # L(W - epsilon)\n",
    "            layer.A[i] = orig - epsilon\n",
    "            L_minus = model.loss_function.loss(model.forward(X), Y)\n",
    "\n",
    "            # Restore original value\n",
    "            layer.A[i] = orig\n",
    "\n",
    "            # Numerical gradient approximation\n",
    "            num_grad_A[i] = (L_plus - L_minus) / (2 * epsilon)\n",
    "            it.iternext()\n",
    "\n",
    "        # --- Check Biases (b) ---\n",
    "        num_grad_b = np.zeros_like(layer.b)\n",
    "        it = np.nditer(layer.b, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            i = it.multi_index\n",
    "            orig = layer.b[i]\n",
    "\n",
    "            # L(b + epsilon)\n",
    "            layer.b[i] = orig + epsilon\n",
    "            L_plus = model.loss_function.loss(model.forward(X), Y)\n",
    "\n",
    "            # L(b - epsilon)\n",
    "            layer.b[i] = orig - epsilon\n",
    "            L_minus = model.loss_function.loss(model.forward(X), Y)\n",
    "\n",
    "            # Restore original value\n",
    "            layer.b[i] = orig\n",
    "\n",
    "            # Numerical gradient approximation\n",
    "            num_grad_b[i] = (L_plus - L_minus) / (2 * epsilon)\n",
    "            it.iternext()\n",
    "\n",
    "        # Compare analytical vs numerical (Relative Error)\n",
    "        # Added 1e-12 to denominator to prevent division by zero if all values are zero\n",
    "        diff_A = np.linalg.norm(num_grad_A - layer.dA) / (np.linalg.norm(num_grad_A) + np.linalg.norm(layer.dA) + 1e-12)\n",
    "        diff_b = np.linalg.norm(num_grad_b - layer.db) / (np.linalg.norm(num_grad_b) + np.linalg.norm(layer.db) + 1e-12)\n",
    "\n",
    "        print(f\"  Weight Gradient Diff (A): {diff_A:.10f}\")\n",
    "        print(f\"  Bias Gradient Diff (b):   {diff_b:.10f}\")\n",
    "\n",
    "        if diff_A < 1e-7 and diff_b < 1e-7:\n",
    "            print(\"  Status: Backprop Correct ✅\")\n",
    "        else:\n",
    "            print(\"  Status: Check Gradients ⚠️ (Error too high)\")\n",
    "\n",
    "# --- Build Simple Test Network for Gradient Check (2-2-1) ---\n",
    "grad_check_model = Sequential()\n",
    "grad_check_model.add(Dense(input_size=2, output_size=2, init_scale=0.1))\n",
    "grad_check_model.add(Tanh())\n",
    "grad_check_model.add(Dense(input_size=2, output_size=1, init_scale=0.1))\n",
    "grad_check_model.add(Tanh())\n",
    "grad_check_model.compile(optimizer=SGD(learning_rate=0.01), loss_function=MeanSquaredError())\n",
    "\n",
    "# --- Run Gradient Check ---\n",
    "# Using a single sample for simplicity\n",
    "X_check = np.array([[-1.0], [0.5]]) \n",
    "Y_check = np.array([[-1.0]])       \n",
    "\n",
    "print(\"Starting Gradient Check on Test Network...\")\n",
    "check_gradients(grad_check_model, X_check, Y_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40481fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XOR Dataset (Using -1 and 1) ---\n",
    "X_xor = np.array([[-1, -1,  1,  1],  # Input: 2 features, 4 samples\n",
    "                  [-1,  1, -1,  1]])\n",
    "\n",
    "Y_xor = np.array([[-1,  1,  1, -1]]) # Target: -1 or 1\n",
    "\n",
    "# --- Build XOR Network (2-4-1) ---\n",
    "xor_model = Sequential()\n",
    "xor_model.add(Dense(input_size=2, output_size=4, init_scale=0.1))\n",
    "xor_model.add(Tanh()) # Hidden Layer Activation\n",
    "xor_model.add(Dense(input_size=4, output_size=1, init_scale=0.1))\n",
    "xor_model.add(Tanh()) # Output Layer Activation (suitable for -1/1 targets)\n",
    "\n",
    "# --- Training Hyperparameters & Compile ---\n",
    "LR = 0.5 # Higher learning rate for faster convergence\n",
    "EPOCHS = 10000 \n",
    "xor_model.compile(optimizer=SGD(learning_rate=LR), loss_function=MeanSquaredError())\n",
    "\n",
    "\n",
    "# --- Run Training ---\n",
    "print(\"\\n==============================\")\n",
    "print(\"Starting XOR Training (2-4-1)\")\n",
    "print(\"==============================\")\n",
    "print(f\"Hyperparameters: LR={LR}, Epochs={EPOCHS}\")\n",
    "\n",
    "loss_history = xor_model.train(X_xor, Y_xor, epochs=EPOCHS)\n",
    "\n",
    "\n",
    "# --- Predictions ---\n",
    "Y_pred_raw = xor_model.predict(X_xor)\n",
    "# Classification for -1/1 targets: values >= 0 classify as 1, values < 0 classify as -1\n",
    "Y_pred_class = np.where(Y_pred_raw >= 0, 1, -1)\n",
    "\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nFinal XOR Predictions:\")\n",
    "print(\"------------------------\")\n",
    "print(\"Input (X):\\n\", X_xor.T)\n",
    "print(\"Target (Y):\\n\", Y_xor.T)\n",
    "print(\"\\nRaw Prediction (Tanh Output):\\n\", Y_pred_raw.T)\n",
    "print(\"Classified Prediction:\\n\", Y_pred_class.T)\n",
    "print(f\"\\nFinal Loss: {loss_history[-1]:.6f}\")\n",
    "\n",
    "\n",
    "# --- Plot Loss Curve ---\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"XOR Training Loss Curve (MSE)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion: If the final loss is near zero and classified predictions match the target, the network successfully solved XOR.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
